{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Facial Classification Using SVMs and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction on Data Using PCA\n",
    "\n",
    "In the 1980's', Alex 'Sandy' Pentland came up with 'EigenFaces', a novel way (at the time) to classify images of faces using dimensionality reduction techniques. The main motivation behind using PCA in eigenfaces was to remove noise from high dimensional data. After transforming the original images, they would train a classifier on these **eigenfaces**.\n",
    "\n",
    "In this assignment, we are going to try replicate the experiment. To get you started, we'll be going through an example using a famous public dataset, the [LFW Dataset](http://vis-www.cs.umass.edu/lfw/).\n",
    "\n",
    "We have loaded the face dataset for you below (As for other famous datasets, sklearn has graciously included this one in their library). \n",
    "\n",
    "The task can be divided in two main steps, described below: \n",
    "\n",
    "- **Feature Extraction -** Use PCA to reduce the number of features required to describe each image (feature extraction step). How many features are required to describe each image while retaining necessary information to describe the image?\n",
    "\n",
    "\n",
    "- **Classification on Extracted Features (Eigenfaces) -** Use a classification algorithm of your choice to train a classifier on the dataset. You may use classification models from sklearn (Hint: are we working on data with high dimensionality? Do we have a few training samples or many training samples? Use [this link](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) for some tips!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import some packages for plotting, matrix operations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Run the cell below, uncommenting the statement that downloads the dataset.\n",
    "\n",
    "In the ```fetch_lfw_people``` method, notice that we choose to resize the image and have a minimum of 70 training instances per person. How many features are there per sample? (**Hint:** Look at the shape of each sample below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the faces dataset\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "# uncomment below to load dataset(takes ~5 mins to load data)\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# assigning features vectors\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot an example image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[1].reshape(h,w), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1) Split the data into train/test splits using a 75/25 split (You may use the sci-kit learn module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Split into train/test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Compute the principal components on the LFW dataset\n",
    "\n",
    "Use principal component analysis to compute appropriate **eigenfaces** on the LFW dataset. As described from the lecture, this is an **unsupervised** learning task for:\n",
    " - feature extraction\n",
    " - dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2) Data Preprocessing\n",
    "\n",
    "Use the ```StandardScaler``` class from sklearn to scale data, and to subtract the mean from each sample [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
    "\n",
    "**NOTE:** Remember to run **ALL** preprocessing steps on training data on your test data before making a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Preprocess training data using the StandardScaler module\n",
    "\n",
    "### YOUR CODE HERE - Preprocess test data using the StandardScaler module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out some values of the first preprocessed instance (rename ```X_train``` if you changed the variable name, or else the following code will throw an error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scaled pixel values of first sample in X_train: {}\".format(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3) Compute PCA, specifying the number of components to compute PCA on.\n",
    "\n",
    "How do we choose the number of principal components? Choose different values of k, and compute the principal components for each iteration (**Hint:** run a for loop over different values of k). Specifically:\n",
    "\n",
    "- Create an empty list that will store values of variance per component.\n",
    "- Iterate from 1 component to k components, transforming the original images in your training set to their principal components (**Hint:** use the ```fit_transform``` method  found [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA))\n",
    "- In the same loop, once you've computed the PCA for ```n_components=k```, store the **last element** of the ```pca.explained_variance_ratio``` attribute in the list you created.\n",
    "- Plot the **percentage of variance** explained by every vector in descending order.\n",
    "\n",
    "Your plot should looking something like this (just a bar chart will suffice): [![PCA](https://miro.medium.com/max/700/1*JLAVaWW5609YZoJ-NYkSOA.png)](https://towardsdatascience.com/a-step-by-step-explanation-of-principal-component-analysis-b836fb9c97e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Specify a range of values of k to check, e.g. values = range(1, 10)\n",
    "\n",
    "### YOUR CODE HERE - Loop over values of k, apply PCA with k components to transform your training data\n",
    "\n",
    "    # Find k components with highest variance\n",
    "    \n",
    "    # Store last element of explained variance ratio in list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4) Plot variance fraction explained by each component/vector in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Plot variance explained by every vector in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5) Choose a specific number of components to use when applying PCA.\n",
    "\n",
    "Now that you have a clearer picture of how many components are required to describe the variance in a given image, apply PCA to the training set, using the sklearn API:\n",
    "```\n",
    "# Some example code on how to find PCA of data\n",
    "\n",
    "pca = decomposition.PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "\n",
    "# How to reshape PCA components to plot later (h, w defined above)\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "```\n",
    "Compute eigenfaces, then project the input data onto the orthonormal based on the example code provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Find eigenfaces from training data\n",
    "\n",
    "### YOUR CODE HERE - Project input data on the eigenfaces orthonormal basis using the transform command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6) Train an SVM classification model on the transformed data \n",
    "\n",
    "Find an appropriate SVM model to classify our transformed data on using hyperparameter tuning. Print out the best model, then apply the best model to your test data.\n",
    "\n",
    "- Specify a parameter grid dictionary to perform a grid search over (hyperparameter tuning): specify a grid from parameters 'C', and 'gamma'.\n",
    "- Try different kernels when initializing the classifier, e.g. ```clf = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=2)```\n",
    "- **Print out the best model** using the ```best_estimator_``` attribute.\n",
    "- Evaluate your best model on the test data, using the ```classification_report ``` and ```confusion_matrix ``` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Train an SVM classification model, performing a hyper-parameter search to tune the model\n",
    "\n",
    "### YOUR CODE HERE - Make predictions on the test set (remember to transform your test data)\n",
    "\n",
    "### YOUR CODE HERE - Print out classification report, confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 7) Plot some of the predictions using the following helper methods\n",
    "\n",
    "These methods help with visualization of predictions and the computed eigenfaces. Follow the **example code below** to call the methods on your predictions and plot a few of them.\n",
    "\n",
    "```\n",
    "# Get prediction titles using the title method\n",
    "prediction_titles = [title(y_pred, y_test, target_names, i)\n",
    "                     for i in range(y_pred.shape[0])]\n",
    "\n",
    "plot_gallery(X_test, prediction_titles, h, w)\n",
    "\n",
    "\n",
    "# plot the gallery of the most significative eigenfaces\n",
    "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "        \n",
    "# plot the result of the prediction on a portion of the test set\n",
    "\n",
    "def title(y_pred, y_test, target_names, i):\n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
    "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
    "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Plot predictions and some eigenfaces\n",
    "\n",
    "### YOUR CODE HERE - plot the gallery of the most significative eigenfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS - Train 3 other classification algorithms on eigenfaces data\n",
    "\n",
    "Congratulations on building your first classification algorithm using eigenfaces! As not algorithms perform as well on a given dataset, it's always great to build experience by comparing multiple algorithms using evaluation techniques such as cross-validation (as seen in the previous assignment). What algorithms work better, and why do you think so?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
