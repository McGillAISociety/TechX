{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Intro To Keras With Feed Forward NNs and Theory Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART I - THEORY RECAP\n",
    "\n",
    "Answer the following review questions below. The questions are based off material seen in previous lectures/assignments, but some concepts were only briefly mentioned! If you can't find an answer from the lecture slides, a quick search online should be very helpful (i.e. always search online first before panicking :) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Linear vs Polynomial Regression\n",
    "- Describe both Linear Regression and Polynomial Regression (3 lines or less each).\n",
    "\n",
    "- Describe overfitting vs underfitting with respect to parameters.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE - YOUR MAY USE MARKDOWN, LATEX, CODE, DIAGRAMS, ETC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2) Logistic Regression vs. Linear SVM\n",
    "- Describe how logistic regression works (3 lines or less)\n",
    "- Describe how linear SVM works. Mention the role(s) of:\n",
    "    - support vectors\n",
    "    - margin\n",
    "    - slack variables\n",
    "    - kernels\n",
    "- Plot an example for SVM where the linear kernel is not enough to separate the data, but another kernel works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR ANSWER HERE - YOUR MAY USE MARKDOWN, LATEX, CODE, DIAGRAMS, ETC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Linear SVM vs k-NN\n",
    "- K-Nearest Neighbours is a popular unsupervised learning algorithm. Explain the difference between supervised and unsupervised learning?\n",
    "- K-NN is an example of a lazy learning algorithm. Why is it called so. What could be a use case? Justify using a lazy learning algorithm in that case.\n",
    "- Outline the main steps for the KNN algorithm. Use text, code, plots, diagrams, etc as necessary.  \n",
    "- Plot a example dataset which works in an SVM classification and not k-NN classification. Repeat for the reverse scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR ANSWER HERE - YOUR MAY USE MARKDOWN, LATEX, CODE, DIAGRAMS, ETC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Ensemble Methods\n",
    "- Explain bagging and boosting. Clearly illustrate the difference between these methods. When would you use either one?\n",
    "- What is a decision tree? What is a random forest? Compare them and list 3 pros and cons of each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR ANSWER HERE - YOUR MAY USE MARKDOWN, LATEX, CODE, DIAGRAMS, ETC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) PCA\n",
    "- Describe how PCA achieves dimensionality reduction. Outline the main steps of the algorithm\n",
    "- What is the importance of eigenvectors and eigenvalues in the PCA algorithm above.\n",
    "- When we compute the covariance matrix in PCA, we have to subtract the mean. Why do we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR ANSWER HERE - YOUR MAY USE MARKDOWN, LATEX, CODE, DIAGRAMS, ETC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART II - INTRODUCTION TO THE KERAS LIBRARY\n",
    "\n",
    "In the following, we will be introduced to the [Keras Deep Learning Library](https://keras.io/). The library is a high-level API to get you started with building your own neural networks! In this tutorial, we'll be building a feed-forward neural network, and in the next assignment you'll build different CNN architectures.\n",
    "\n",
    "The essence of Keras revolves around different Keras **models**. The simplest type of model is the ```Sequential``` model, a **linear** stack of layers. For more complex architectures, you should use the [Keras functional API](https://keras.io/getting-started/functional-api-guide/), which allows to build arbitrary graphs of layers.\n",
    "\n",
    "Below are a few of the key commands to get you started right away!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get rid of some warning logs, you can run the following helper method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cell to get rid of some warnings later on\n",
    "\n",
    "def noWarnings():\n",
    "    import os\n",
    "    os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '0'\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "    from tensorflow import logging\n",
    "    logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1) Create a Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Stacking Layers\n",
    "\n",
    "Stacking layers is as easy as ```.add()```:\n",
    "\n",
    "Note that for the first layer, we must specify the ```input_dim``` parameter since the first layer of a linearly stacked neural network feeds each input feature to its respective neuron.\n",
    "\n",
    "You must also specify the number of units per layer, as well as specify an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2) Add 2 layers to the model.\n",
    "\n",
    "- an input layer of 64 neurons using the ReLU activation function, with input dimension of 100\n",
    "- a layer of 10 units with the softmax activation function (output layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3) Specify the model's learning process\n",
    "\n",
    "Before training, we have to use the ```.compile()``` method which specifies the model's learning process. To create a learning process, the following parameters need to be specified:\n",
    "\n",
    "- a loss function, e.g. categorical cross entropy for multiclass classification\n",
    "- an optimization method, e.g. stochastic gradient descent\n",
    "- a metric to optimize for, e.g. accuracy for classification\n",
    "\n",
    "**TIP**: custom loss functions can be daunting (typically specific tasks will have their own custom loss functions), the [Keras Losses Documentation](https://keras.io/losses/) shows a few loss functions available in the API, read up online to see if the loss function is suitable for the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to, you can further configure your optimizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4) You can now iterate on your training data in batches (following code will not run unless you specify X_train, y_train)\n",
    "\n",
    "Note the similarity with sklearn's ```.fit()``` command to train a model! Note that the ```epochs``` parameter denotes the number of times the model will pass over the training set during the training phase ([More info here](https://keras.io/getting-started/faq/#what-does-sample-batch-epoch-mean))\n",
    "\n",
    "```\n",
    "\n",
    "# X_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Evaluating Model Performance\n",
    "\n",
    "To evaluate model performance, use the ```.evaluate()``` command:\n",
    "\n",
    "```\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=128)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6) ...or make new predictions on test data!\n",
    "\n",
    "Use the ```.predict()``` command:\n",
    "\n",
    "```\n",
    "\n",
    "classes = model.predict(x_test, batch_size=128)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART III - Building a Deep Neural Network for Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a multi-class classification problem, picking 3 classes for demonstration, but our approach generalizes to any number of classes.\n",
    "\n",
    "To see how deep neural networks are helpful compared to other traditional supervised learning algorithms, we'll be inspecting a dataset that is not linearly separable. We'll then try to solve the problem using **softmax** regression in Keras, which is extremely similar to logistic regression, but is used for multiclass problems.\n",
    "\n",
    "We'll then attempt to tackle the same problem using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1) Import the following libraries for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2) Some helper methods\n",
    "\n",
    "Here are some helper methods to generate datasets and to help create visually appealing plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multiclass(N=500, D=2, K=3):\n",
    "    \"\"\"\n",
    "    N: number of points per class\n",
    "    D: dimensionality\n",
    "    K: number of classes\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    X = np.zeros((N*K, D))\n",
    "    y = np.zeros(N*K)\n",
    "    for j in range(K):\n",
    "        ix = range(N*j, N*(j+1))\n",
    "        # radius\n",
    "        r = np.linspace(0.0,1,N)\n",
    "        # theta\n",
    "        t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu, alpha=0.8)\n",
    "    plt.xlim([-1,1])\n",
    "    plt.ylim([-1,1])\n",
    "    return X, y\n",
    "\n",
    "def plot_multiclass_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
    "    cmap = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "    Z = model.predict_classes(np.c_[xx.ravel(), yy.ravel()], verbose=0)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "def plot_loss_accuracy(history):\n",
    "    historydf = pd.DataFrame(history.history, index=history.epoch)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    historydf.plot(ylim=(0, max(1, historydf.values.max())))\n",
    "    loss = history.history['loss'][-1]\n",
    "    acc = history.history['acc'][-1]\n",
    "    plt.title('Loss: %.3f, Accuracy: %.3f' % (loss, acc))\n",
    "\n",
    "def plot_confusion_matrix(model, X, y):\n",
    "    y_pred = model.predict_classes(X, verbose=0)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(pd.DataFrame(confusion_matrix(y, y_pred)), annot=True, fmt='d', cmap='YlGnBu', alpha=0.8, vmin=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3) Plot the data generated.\n",
    "\n",
    "Inspect the shape of X, y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_multiclass(K=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4) Building a Logistic Regression Model as a Reference to a Softmax Regression Model\n",
    "\n",
    "Recall the Logistic Regression supervised learning algorithm for binary classification problems. As the problem states, LR works with binary labels 0/1. Softmax Regression (SR) is a generalization of LR where we can have more than 2 classes. In our current dataset we have 3 classes, represented as 0/1/2.\n",
    "\n",
    "Building the model for SR is very similar to LR, for reference here's how a **LR model** is built:\n",
    "\n",
    "```\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(2,), activation='sigmoid'))\n",
    "\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, y, verbose=0, epochs=20)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5) Building a Softmax Regression Model\n",
    "\n",
    "The core idea of building the softmax regression model in Keras is very similar to the LR model in the sample code above. Let's go over a few of the differences to consider when implementing SR for multiclass classification:\n",
    "\n",
    "- **Number of nodes in the dense layer:** LR uses 1 node, while SR uses 3. Since we have 3 classes, it intuitively makes sense for SR to have 3 nodes. Then the question is, why does LR uses only 1 node for 2 classes?\n",
    "\n",
    "The answer is simply because we can achieve the same result with using only 1 node. Recall from previous lectures, LR models the probability of an example belonging to class one: $P(y=1)$. We know the sum of the probabilities in a probability distribution is always 1, so we can calculate class 0 probability by: $1 - P(y=1)$. But when we have more than 2 classes, we need individual nodes for each class (knowing the probability of one class doesn't let us infer the probability of the other classes).\n",
    "\n",
    "\n",
    "- **Activation function:** LR used sigmoid activation function, SR uses *softmax*. It scales the probability of the output neurons such that they sum up to 1. So in our case $P(y=0) + P(y=1) + P(y=2) = 1$. For now you can simply think of it as a normalization function which let's us interpret the output values as probabilities.\n",
    "\n",
    "- **Loss function:** In a binary classification problem like LR, the loss function is binary_crossentropy. In the multiclass case like SR, the loss function is categorical_crossentropy. Going into the theory behind loss functions is beyond the scope of this tutorial.\n",
    "\n",
    "- **Fitted data:** LR used the vector y directly in the fit function, which has just one column with binary 0/1 values. When we're applying SR the labels need to be in *one-hot* representation. In our case y_cat is a matrix with 3 columns, where all the values are 0 except for the one that represents our class, which is set to 1 instead (Hint: check the imports)\n",
    "\n",
    "\n",
    "Note: recall that LR is a linear classifier, and so is SR, but for multiple classes. Hence, the \"power\" of the model hasn't changed, it is simply the multiclass extension of LR.\n",
    "\n",
    "When trained correctly, the model gives us an accuracy of around 50%, because the dataset is not linearly separable.\n",
    "\n",
    "**Train an SR model on the dataset generated above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Create a sequential model for SR\n",
    "\n",
    "### YOUR CODE HERE - Train the model on one-hot encoded target labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6) Plot the loss accuracy, decision boundaries learned by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Plot loss accuracy of your model using the plot_loss_accuracy helper method\n",
    "\n",
    "### YOUR CODE HERE - Plot decision boundaries learned using the multi_class_decision_boundary helper method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 7) Make predictions on X, then evaluate how the algorithm does using classification metrics\n",
    "\n",
    "- Use the ```.predict_classes()``` method to generate predictions\n",
    "- Print out metrics using the ```classification_report``` module\n",
    "- Print out the confusion matrix of the model using the ```confusion_matrix``` helper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Evaluate SR performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEEP NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a deep neural network, we only need to add more Dense layers. To make learning easier for the network, we'll try adding a couple of Dense layers with tanh activation function, and decrease number of nodes per layer.\n",
    "\n",
    "To build a more powerful model that can learn non-linearly separable data, we add more layers with non-linear activation functions. Build a classification model in Keras with:\n",
    "\n",
    "- a densely connected layer with 128 units, 'tanh' activation function\n",
    "- a densely connected layer with 64 units, 'tanh' activation function\n",
    "- a densely connected layer with 32 units, 'tanh' activation function\n",
    "- a densely connected layer with 16 units, 'tanh' activation function\n",
    "- a densely connected layer with 3 units, 'softmax' activation function\n",
    "\n",
    "Train your model on the same dataset, defining the learning process similarly to the LR sample code. How many epochs are required to train an accurate model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 8) Build a deep NN model using Keras, fit it on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Build a Deep Neural Network model and compile it\n",
    "\n",
    "### YOUR CODE HERE - Fit data on training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 9) Plot the loss accuracy, decision boundaries learned by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Plot loss accuracy of your model using the plot_loss_accuracy helper method\n",
    "\n",
    "### YOUR CODE HERE - Plot decision boundaries learned using the multi_class_decision_boundary helper method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10) Make predictions on X, then evaluate how the algorithm does using classification metrics\n",
    "\n",
    "- Use the ```.predict_classes()``` method to generate predictions\n",
    "- Print out metrics using the ```classification_report``` module\n",
    "- Print out the confusion matrix of the model using the ```confusion_matrix``` helper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Evaluate SR performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 11) Comment on model performance.\n",
    "\n",
    "Compare the SR classification performance compare to the ANN's using the classification metrics, loss, etc. Does the neural network perform better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ========== YOUR ANSWER HERE ========= ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
