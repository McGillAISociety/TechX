{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: K-NNs and Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, answer directly on this Jupyter notebook. Once you're done, please submit the assignment as \"Name_Surname_Assignment5.ipynb\"\n",
    "\n",
    "*Don't forget that commenting your code is very important!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "###  K-NN Implementation\n",
    "\n",
    "- Implement the K-NN algorithm by hand (ie. Don't use the sklearn implementation).\n",
    "- Evaluate and plot model performance for different values of k.\n",
    "\n",
    "For this question, we will be using the classic Iris dataset, available in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import packages\n",
    "\n",
    "###### Importing packages and knowing what packages you need for a project is crucial. We will not be reminding you which packages you need for each question and for the assignment in general. Please import the packages at your own discretion. Although it is common practice to import all packages at once at the beginning, don't hesitate to revisit the next cell, and import more packages as you may need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement kNN by hand. It might be useful to store all distances in one array/list\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import operator\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= iris['feature_names'] + ['target'])\n",
    "\n",
    "# Preview dataset\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In order to evaluate the performance of our kNN implementation, we first split the dataset into training and test sets. A 70/30 split or something similar should suffice. Remember class balance within both train/test sets is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Shuffle dataset, then split for balanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining a distance metric\n",
    "\n",
    "###### To define similarity between two given points, we must define a distance metric. Write a method that takes  two points as input and returns the distance between the points. Look at the format of each point in the sample case below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Write method that returns Euclidean distance between two points, last element in point array is class \n",
    "\n",
    "def getDistance(p1, p2):\n",
    "    \"\"\" \n",
    "    p1:  a numpy array, last element of the array is the class of the point\n",
    "    p2:  a numpy array, last element of the array is the class of the point\n",
    "    Calculates the Euclidean distance between two points. Returns dist float\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Let's test our newly written method on the following samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [2, 2, 2, 'a']\n",
    "data2 = [4, 4, 4, 'b']\n",
    "distance = getDistance(data1, data2)\n",
    "\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Finding k nearest neighbours\n",
    "\n",
    "###### Now that we've defined a distance metric, we can use it collect the k most similar instances for a new test instance. Write a method calculating the distance between a test point and all training instances, selecting a subset with the smallest distance values. It might be useful to store all the distances in a list/array, since Python has a built-in sorting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Write method calculating the distance for all instances, selecting a subset with the smallest distance values.\n",
    "\n",
    "def getNeighbours(trainingSet, sample, k):\n",
    "    \"\"\"\n",
    "    trainingSet: a list of points, where each point is a numpy array and the last element of the array is the class of the point\n",
    "    sample: a numpy array, last element of the array is the class of the point\n",
    "    k: positive integer \n",
    "    \n",
    "    Calculates k nearest neighbours using a distance metric. \n",
    "    \n",
    "    Returns neighbours list\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Let's test our newly written method on the following samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = [[2, 2, 2, 'a'], [4, 4, 4, 'b']]\n",
    "testInstance = [5, 5, 5]\n",
    "\n",
    "neighbors = getNeighbours(trainSet, testInstance, 1)\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now to build a prediction model, write a method that returns a prediction given k nearest neighbours from the previous method. (Hint: one way you can do this is to build a dictionary, and sort the key-value pairs to determine which class occurs the most often!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Write method that takes in k nearest neighbours as input, and votes based on the majority class.\n",
    "\n",
    "def predict(neighbours):\n",
    "    \"\"\" \n",
    "    neighbours: a list of points, where each point is a numpy array and the last element of the array is the class of the point\n",
    "    \n",
    "    Returns predicted class response in string format based off majority vote from k neighbours set \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test your method on the following samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = [[1,1,1,'a'], [2,2,2,'a'], [3,3,3,'b']]\n",
    "response = predict(neighbors)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Model Performance\n",
    "\n",
    "######  We're basically ready to test the performance of our very own k-NN implementation! One popular classification metric is accuracy, use the following method to check how well our k-NN algorithm performs on the test set we left aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for x in range(len(testSet)):\n",
    "        if (testSet[x][-1] == predictions[x]):\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet))) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the method on the following samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSet = [[1,1,1,'a'], [2,2,2,'a'], [3,3,3,'b']]\n",
    "predictions = ['a', 'a', 'a']\n",
    "accuracy = getAccuracy(testSet, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Make predictions on the test set for different values of k. Compare the accuracy for different values and plot test performance, explaining why you think performance increases or decreases for different values of k (WRITE ANSWER IN BOX BELOW PLOT). Loop from k=1 to k=49 (inclusively), only considering cases where k is odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Create a dictionary to store accuracies for different values of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot a bar chart, having the different values of k on the x-axis and the test accuracy (in %) on the y-axis. Add a plot title, and choose a suitable format for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Create a bar plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #F9F2EB\"> YOUR EXPLANATION HERE...  </span>\n",
    "*(Hint: think of the bias-variance trade-off/ AND underfitting/overfitting)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Classification Algorithm Comparison\n",
    "\n",
    "Now that we've learned about a couple of classification algorithms, you must be wondering: which algorithm should I choose for a given task? In the following question, we'll learn to investigate model performance for various algorithms and then based off the best algorithm, do some hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1) Import necessary packages from sklearn.\n",
    "\n",
    "Here are a few popular classification algorithms available from the sklearn API. If you're curious on what supervised learning algorithms are available, don't hesitate to read up on documentation: https://scikit-learn.org/stable/supervised_learning.html (this might be useful brainstorming for your hackathon idea!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2) Load the Pima Indians Diabetes Dataset using pandas.\n",
    "\n",
    "Just another way of downloading a CSV file from online! Let's inspect the dataset we're working with, the Pima Indians Diabetes Dataset.\n",
    "\n",
    "\" This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. \" - Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "df = pd.read_csv(url, names=names)  # names parameter is simply used to name the columns, defaults to None if unspecified\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3) Take the values of the DataFrame as a numpy array, then separate the features and target variable columns into X, y respectively.\n",
    "\n",
    "We don't directly split up the train/test splits, because we'll be using the sklearn k cross-fold validation implementation, which takes in the entire dataset as an argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df.values\n",
    "print(array.shape)\n",
    "\n",
    "X = array[:,0:8]  # Take first 8 columns as features\n",
    "y = array[:,8]  # Take last column as the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're going to fix the random seed in the data partitioning step of k cross-fold validation, for the simple reason of wanting to minimize performance differences to be due to different data partitions seen by each algorithm. Hence, adding this seed will cause every algorithm to see the same shuffled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed in data partitions\n",
    "seed = 7\n",
    "\n",
    "# Prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='lbfgs', max_iter=500)))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=100)))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4) Now, we evaluate each model individually and store cross-validation results for preliminary comparison. Note that we haven't done any hyperparameter tuning as we have for our k-NN implementation previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model individually\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in models:\n",
    "    \n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)  # Provides train/test indices to split data in train/test sets.\n",
    "    cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)  \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"{}: {} ({})\".format(name, cv_results.mean(), cv_results.std())  # Provides the mean accuracy, std dev for results\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5) Use a boxplot to show results\n",
    "\n",
    "In both academia/research and industry, graphs are necessary to simply display results and convey messages more clearly. Below, we show a boxplot, where each horizontal line represents the median result obtained, the T-shaped ends show the highest and lowest results. The box simply contains where \"most\" the results lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, it's your turn!\n",
    "\n",
    "After doing some preliminary comparison between a few classification algorithms, you can now select a few and do some hyperparameter tuning. For example, random forests did fairly well, and we haven't even tuned any of its hyperparameters (e.g. max tree depth, number of trees, etc.) To find optimal hyperparameters, it's common to perform a **grid search** over a parameter space. Essentially, you want to specify a range of values for different hyperparameters within an algorithm, and try out every combination of parameters that are evaluated on a test set left aside (Do you see the recurring theme of leaving data aside?). Typically, you'd select the hyperparameter values that worked best on your test set. \n",
    "\n",
    "Sklearn has an API for just that, which takes in a **model**, and a **dictionary of parameters** to search over: [scikit-learn docs](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) . Follow the example [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py) to see how to use the API:\n",
    "- Select an algorithm of your choice from the sklearn classification models (Random Forests is a nice one though)\n",
    "- Used the **GridSearchCV** module to find optimal hyperparameters for your model (pick 2-3 parameters to search over!)\n",
    "- Use classification report to show model performance\n",
    "- Let us know if you have any cool results to show!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE - Find optimal hyperparameters for an algorithm of your choice\n",
    "\n",
    "### YOUR CODE HERE - Split the dataset in two equal parts\n",
    "\n",
    "### YOUR CODE HERE - Set the parameters by cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now you're familiar with fundamental, but VERY important algorithm selection and evaluation techniques! Congrats :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
